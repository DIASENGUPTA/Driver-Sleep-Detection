{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "scenic-daily",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "Training set (3938, 24, 24, 1) (3938, 1)\n",
      "Test set (986, 24, 24, 1) (986, 1)\n",
      "3938 train samples, 1 channels, 24x24\n",
      "986  test samples, 1 channels, 24x24\n",
      "Epoch 1/12\n",
      "132/132 - 11s - loss: 0.6886 - accuracy: 0.5302 - val_loss: 0.6852 - val_accuracy: 0.5467\n",
      "Epoch 2/12\n",
      "132/132 - 10s - loss: 0.6301 - accuracy: 0.6511 - val_loss: 0.6415 - val_accuracy: 0.6247\n",
      "Epoch 3/12\n",
      "132/132 - 10s - loss: 0.6120 - accuracy: 0.6658 - val_loss: 0.6217 - val_accuracy: 0.6531\n",
      "Epoch 4/12\n",
      "132/132 - 10s - loss: 0.5883 - accuracy: 0.6861 - val_loss: 0.5860 - val_accuracy: 0.6795\n",
      "Epoch 5/12\n",
      "132/132 - 10s - loss: 0.5282 - accuracy: 0.7367 - val_loss: 0.4708 - val_accuracy: 0.7454\n",
      "Epoch 6/12\n",
      "132/132 - 10s - loss: 0.4084 - accuracy: 0.8200 - val_loss: 0.3404 - val_accuracy: 0.8306\n",
      "Epoch 7/12\n",
      "132/132 - 10s - loss: 0.2841 - accuracy: 0.8857 - val_loss: 0.2039 - val_accuracy: 0.9351\n",
      "Epoch 8/12\n",
      "132/132 - 10s - loss: 0.2259 - accuracy: 0.9152 - val_loss: 0.2241 - val_accuracy: 0.9158\n",
      "Epoch 9/12\n",
      "132/132 - 9s - loss: 0.2037 - accuracy: 0.9182 - val_loss: 0.1533 - val_accuracy: 0.9544\n",
      "Epoch 10/12\n",
      "132/132 - 10s - loss: 0.1751 - accuracy: 0.9342 - val_loss: 0.1618 - val_accuracy: 0.9523\n",
      "Epoch 11/12\n",
      "132/132 - 10s - loss: 0.1666 - accuracy: 0.9373 - val_loss: 0.1344 - val_accuracy: 0.9584\n",
      "Epoch 12/12\n",
      "132/132 - 10s - loss: 0.1452 - accuracy: 0.9446 - val_loss: 0.1726 - val_accuracy: 0.9432\n",
      "Loss score: 0.17257331311702728\n",
      "Test accuracy: 94.32048797607422 %\n",
      "Save model to file json...\n",
      "Save weights to file...\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# Main\n",
    "if __name__ == '__main__':\n",
    "    pickle_files = ['open_eyes.pickle', 'closed_eyes.pickle']\n",
    "    i = 0\n",
    "    for pickle_file in pickle_files:\n",
    "        with open(pickle_file, 'rb') as f:\n",
    "            save = pickle.load(f)\n",
    "            if i == 0:\n",
    "                train_dataset = save['train_dataset']\n",
    "                train_labels = save['train_labels']\n",
    "                test_dataset = save['test_dataset']\n",
    "                test_labels = save['test_labels']\n",
    "            else:\n",
    "                print(\"here\")\n",
    "                train_dataset = np.concatenate((train_dataset, save['train_dataset']))\n",
    "                train_labels = np.concatenate((train_labels, save['train_labels']))\n",
    "                test_dataset = np.concatenate((test_dataset, save['test_dataset']))\n",
    "                test_labels = np.concatenate((test_labels, save['test_labels']))\n",
    "            del save  # hint to help gc free up memory\n",
    "        i += 1\n",
    "\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)\n",
    "\n",
    "    batch_size = 30\n",
    "    nb_classes = 1\n",
    "    nb_epoch = 12\n",
    "\n",
    "    X_train = train_dataset\n",
    "    # X_train = X_train.reshape((X_train.shape[0], X_train.shape[3]) + X_train.shape[1:3])\n",
    "    Y_train = train_labels\n",
    "\n",
    "    X_test = test_dataset\n",
    "    # X_test = X_test.reshape((X_test.shape[0], X_test.shape[3]) + X_test.shape[1:3])\n",
    "    Y_test = test_labels\n",
    "\n",
    "    # print data shape\n",
    "    print(\"{1} train samples, {4} channel{0}, {2}x{3}\".format(\"\" if X_train.shape[1] == 1 else \"s\", *X_train.shape))\n",
    "    print(\"{1}  test samples, {4} channel{0}, {2}x{3}\".format(\"\" if X_test.shape[1] == 1 else \"s\", *X_test.shape))\n",
    "    # input image dimensions\n",
    "    _, img_channels, img_rows, img_cols = X_train.shape\n",
    "\n",
    "    model = Sequential()\n",
    "    # first\tand\tsecond Conv Layers with pooling\n",
    "    model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                            input_shape=(img_channels, img_rows, img_cols)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(128, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    # FC layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Output layer. Define the class\n",
    "    model.add(Dense(nb_classes))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    # let's train the model using SGD + momentum (how original).\n",
    "    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=2, validation_data=(X_test, Y_test))\n",
    "\n",
    "    score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "\n",
    "    print('Loss score:', score[0])\n",
    "    print('Test accuracy:', score[1] * 100, '%')\n",
    "\n",
    "    # Save model to file\n",
    "    now = time.time()\n",
    "    print(\"Save model to file json...\")\n",
    "    model_json = model.to_json()\n",
    "    with open('trained_model/model_' + str(now) + '.json', \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "    print(\"Save weights to file...\")\n",
    "    model.save_weights('trained_model/weight_' + str(now) + '.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-nurse",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
